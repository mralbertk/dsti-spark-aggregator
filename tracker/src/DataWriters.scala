import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.functions._

object DataWriters {

	case class WriteCsvFile(spark: SparkSession) {

		val tempPath = "./data/temp/"
		val finalPath = "./data/out"

		def writeFile(data: DataFrame, fileName: String) = {
			data.coalesce(1).write.option("header", true).mode("overwrite").csv(tempPath)

			// Rename part-0000 file to desired name
			import org.apache.hadoop.fs.{FileSystem, Path}
			val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
      		val tempName = fs.globStatus(new Path(s"$tempPath/*.csv"))(0).getPath().getName()
      		fs.rename(new Path(tempPath + tempName), new Path(s"$finalPath/$fileName"))

      		// Delete temporary files generated by Spark
      		import scala.reflect.io.Directory
      		import java.io.File
      		val tempDir = new Directory(new File(tempPath))
      		tempDir.deleteRecursively()
      		for {
      		  files <- Option(new File(finalPath).listFiles)
      		  file <- files if file.getName.endsWith("crc")
      		} file.delete()
		}

	}

}